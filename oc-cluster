#!/bin/bash

OPENSHIFT_HOME_DIR=${OPENSHIFT_HOME:-$HOME/.oc}
OPENSHIFT_CACHE_DIR=${OPENSHIFT_CACHE:-/var/lib/origin}

function up {
  local _profile="$1"
  echo "$_profile"

  if [ "$_profile" == "" ]
  then
     echo "Using default profile"
     _profile="default"
  fi

  # This is where oc cluster stores it's data
  local OPENSHIFT_HOST_DATA_DIR=$OPENSHIFT_HOME_DIR/profiles/$_profile/data
  local OPENSHIFT_HOST_CONFIG_DIR=$OPENSHIFT_HOME_DIR/profiles/$_profile/config
  local OPENSHIFT_HOST_VOLUMES_DIR=$OPENSHIFT_HOME_DIR/profiles/$_profile/volumes

  if test ! -d $OPENSHIFT_HOST_CONFIG_DIR; then
     mkdir -p $OPENSHIFT_HOST_CONFIG_DIR
  fi

  if test ! -d $OPENSHIFT_HOST_DATA_DIR; then
    mkdir -p $OPENSHIFT_HOST_DATA_DIR
  fi

  if test ! -d $OPENSHIFT_HOST_VOLUMES_DIR; then
    mkdir -p $OPENSHIFT_HOST_VOLUMES_DIR
  fi

  shift # Remove profile name

  # Create the profile markfile
  echo "${_profile}" > $OPENSHIFT_HOME_DIR/active_profile

  echo "oc cluster up --public-hostname 127.0.0.1 \
                --host-data-dir $OPENSHIFT_HOST_DATA_DIR \
                --host-config-dir $OPENSHIFT_HOST_CONFIG_DIR \
                --use-existing-config \
                $@"
  oc cluster up --public-hostname 127.0.0.1 \
                --host-data-dir $OPENSHIFT_HOST_DATA_DIR \
                --host-config-dir $OPENSHIFT_HOST_CONFIG_DIR \
                --use-existing-config \
                "$@"
  
  #Add developer as sudoer
  oc adm policy add-cluster-role-to-group sudoer system:authenticated \
         --config="${OPENSHIFT_HOST_CONFIG_DIR}/master/admin.kubeconfig" \
         --context="default/127-0-0-1:8443/system:admin"

  echo "Any user is sudoer. They can execute commands with '--as=system:admin'"
  # Create user admin that can log into the console
  oc adm policy add-cluster-role-to-user cluster-admin admin --as=system:admin
}

function down {
  status -s
  local _status=$?
  if [ $_status -ne 0 ]
  then  
    echo "Bringing the cluster down"
    oc cluster down
    rm -f $OPENSHIFT_HOME_DIR/active_profile
  fi
}

#
# Args:
#  $1: [-s] silent
function status {
  local _silent=$1

  if [[ "$(docker ps -f name=origin -q)" == "" ]]
  then
     if [ "$_silent" != "-s" ]; then
        echo "no cluster running"
     fi
     return 0
  else
     if [ "$_silent" != "-s" ]; then
        echo "oc cluster running. Current profile <$([ -f $OPENSHIFT_HOME_DIR/active_profile ] && cat $OPENSHIFT_HOME_DIR/active_profile || echo 'unknown')>"
     fi
     return 1
  fi
}

# If the cluster is up, it will bring it down and destroy active_profile
# Otherwise will ask for profile 
function destroy {
  local _profile=$1
  local _active_profile=$([ -f $OPENSHIFT_HOME_DIR/active_profile ] && cat $OPENSHIFT_HOME_DIR/active_profile || echo '')

  if [ $# -lt 1 ] && [ "${_active_profile}" == "" ]
  then
    echo "You need to specify a profile, or have a running cluster"
    exit 1
  fi
  [ "$_profile" == "" ] && _profile=$_active_profile

  if [ "$_profile" != "" ]
  then
     read -p "Are you sure you want to destroy cluster with profile <$_profile> (y/n)? " -n 1 -r
     echo    # move to a new line
     if [[ $REPLY =~ ^[Yy]$ ]]
     then
        # do dangerous stuff
        echo "Removing profile $_profile"
        if [ "$_active_profile" == "$_profile" ]
        then
           down
        fi
        echo "Removing $OPENSHIFT_HOME_DIR/profiles/$_profile"
        rm -rf $OPENSHIFT_HOME_DIR/profiles/$_profile
        # TODO: Remove the images
     fi
  fi
}

function list {
  echo "Profiles:"
  for i in `ls $OPENSHIFT_HOME_DIR/profiles`
  do
     echo "- $i"
  done
}

function deploy-nexus {
  status -s
  local _status=$?
  if [ $_status -ne 0 ]
  then
     oc adm new-project ci --as=system:admin
     create-shared-volume 'ci/nexus-data'
     oc create -f https://raw.githubusercontent.com/openshift-evangelists/vagrant-origin/master/utils/nexus/nexus.yaml -n ci --as=system:admin
     oc adm policy add-role-to-user admin $(oc whoami) --as=system:admin -n ci
     echo "Project ci has been created and shared with you. It has a nexus instance that has shared storage with other clusters" 
  fi
}

function ssh {
  echo "Going into the Origin Container"
  docker exec -it origin /bin/bash
}

function console {
  echo "https://127.0.0.1:8443/console"
}

function create-volume {
  local __volume=$1
  local __size=$2
  local __path=$3

  [ $# -lt 1 ] && echo "volumename is required" && exit 1

  __profile=$(cat $OPENSHIFT_HOME_DIR/active_profile)
  local __path=$OPENSHIFT_HOME_DIR/profiles/${__profile}/volumes/${__volume}

  if test ! -d $__path; then
    mkdir -p $__path
  fi

cat <<-EOF > /tmp/pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ${__volume}
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: ${__path}
EOF

oc create -f /tmp/pv.yaml --as=system:admin
rm /tmp/pv.yaml

echo "Volume created in ${__path}"
}

function create-shared-volume {
  [ $# -lt 1 ] && echo "volumename is required. This should take the form of <project>/<name> so the volume will be prebounded to the claim with same name in the specified project" && exit 1

  local __projectclaim=$1
  arrIN=(${__projectclaim//\// })
  [ ${#arrIN[@]} -lt 2 ] && echo "volumename format is not valid.  This should take the form of <project>/<name> so the volume will be prebounded to the claim with same name in the specified project" && exit 1
  
  local __volume=${arrIN[1]}
  local __project=${arrIN[0]}
  local __size=$2
  local __path=$3

  local __path=$OPENSHIFT_HOME_DIR/volumes/${__volume}

  if test ! -d $__path; then
    mkdir -p $__path
  fi
  
  # TODO: Check if volume exists

cat <<-EOF > /tmp/pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ${__volume}
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
    - ReadWriteMany
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    namespace: ${__project}
    name: ${__volume}
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: ${__path}
EOF

oc create -f /tmp/pv.yaml --as=system:admin
rm /tmp/pv.yaml

echo "Volume created in ${__path}"
}

function help {
        echo "Valid commands:"
        echo "oc-cluster up [profile] [OPTIONS]"
        echo "oc-cluster down"
        echo "oc-cluster destroy [profile]"
        echo "oc-cluster list"
        echo "oc-cluster status"
        echo "oc-cluster ssh"
        echo "oc-cluster console"
        echo "oc-cluster create-volume volumeName [size|10Gi] [path|$HOME/.oc/profiles/<profile>/volumes/<volumeName>]"
        echo "oc-cluster create-shared-volume project/volumeName [size|10Gi] [path|$HOME/.oc/volumes/<volumeName>]"
        echo "oc-cluster deploy-nexus"
}

# Use -gt 1 to consume two arguments per pass in the loop (e.g. each
# argument has a corresponding value to go with it).
# Use -gt 0 to consume one or more arguments per pass in the loop (e.g.
# some arguments don't have a corresponding value to go with it such
# as in the --default example).
if [[ $# -gt 0 ]]
then
   key="$1"
   case $key in
      up)
        shift # past argument
        up "$@"
        ;;
      down)
        shift # past argument
        down "$@"
        ;;
      destroy)
        shift # past argument
        destroy "$@"
        ;;
      list)
        shift # past argument
        list "$@"
        ;;
      status)
        shift
        status "$@"
        ;;
      ssh)
        shift
        ssh "$@"
        ;;
      create-volume)
        shift
        create-volume "$@"
        ;;
      create-shared-volume)
        shift
        create-shared-volume "$@"
        ;;
      deploy-nexus)
        shift
        deploy-nexus "$@"
        ;;
      console)
        shift
        console "$@"
        ;;
      -h|help)
        help        
        ;;
      *)
        # unknown option
        echo "Unknown option. Show help"
        help        
        ;;
   esac
else
   help
fi


